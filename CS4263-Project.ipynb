{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Using Google Search Volume and News Sentiment to Predict Natural Gas Prices with LSTMs\n",
    "by Quinn Murphey, Adrian Ramos, and Gabriel Soliz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import skopt as skopt\n",
    "import skopt.plots\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_FOLDER_NAME = \"futures_predictions_with_var_length_nymex_and_google_trends/\"\n",
    "Path(TOP_FOLDER_NAME+\"images\").mkdir(parents=True, exist_ok=True)\n",
    "Path(TOP_FOLDER_NAME+\"models/hyperparam_search\").mkdir(parents=True, exist_ok=True)\n",
    "Path(TOP_FOLDER_NAME+\"models/trained\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "### NYMEX Data Parser\n",
    "NYMEX_START_DATE = datetime(year=2004, month=1, day=1)\n",
    "NYMEX_END_DATE = datetime(year=2005, month=6, day=28)\n",
    "\n",
    "### Google Scraper\n",
    "KEYWORDS            = [\"Natural Gas\",\"Oil\",\"Coal\",\"Nuclear Power\",\"Wind Power\",\"Hydroelectric\",\"Solar Power\",\"Gold\",\"Silver\",\"Platinum\",\"Copper\",\"Biofuel\",\"Recession\",\"CPI\"]\n",
    "KEYWORDS_CATEGORIES = [904,          904,  904,   0,               0,          0,               0,           904,   904,     904,       904,     0,        0,          0]\n",
    "TIMEFRAME='2004-01-01 2019-06-28' # Jan 2013 - June 2019\n",
    "COUNTRIES=[\"US\"] # ISO country code\n",
    "SEARCH_TYPE='' #default is 'web searches',others include 'images','news','youtube','froogle' (google shopping)\n",
    "\n",
    "### TF Dataset Creation\n",
    "FEATURES = ['Spot Price', 'Futures 1 Price', 'Futures 2 Price', 'Futures 3 Price',\n",
    "       'Futures 4 Price', 'Natural Gas', 'Oil', 'Coal', 'Nuclear Power',\n",
    "       'Wind Power', 'Hydroelectric', 'Solar Power', 'Gold', 'Silver',\n",
    "       'Platinum', 'Copper', 'Biofuel', 'Recession', 'CPI']\n",
    "LABELS = ['Futures 1 Price','Futures 2 Price','Futures 3 Price','Futures 4 Price']\n",
    "FEATURE_WIDTH = 7 #only for windowed datasets\n",
    "LABEL_WIDTH   = 1\n",
    "LABEL_DATES = pd.date_range(datetime(year=2013, month=1, day=1), datetime(year=2019, month=6, day=28))\n",
    "\n",
    "#DATASET_TYPE='window'\n",
    "DATASET_TYPE='variable'\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT   = 0.1\n",
    "TEST_SPLIT  = 0.1\n",
    "BATCH_SIZE  = 16\n",
    "REPEATS     = 1\n",
    "\n",
    "### Hyperparam Search\n",
    "MAX_SEARCH = 50\n",
    "MAX_EPOCHS = 25\n",
    "INPUT_SHAPE = (None, len(FEATURES))\n",
    "OUTPUT_SHAPE = (LABEL_WIDTH, len(LABELS))\n",
    "\n",
    "SEARCH_STACKED_LSTM_HYPERPARAMS = True\n",
    "SEARCH_STACKED_BILSTM_HYPERPARAMS = True\n",
    "SEARCH_ENSEMBLE_STACKED_BILSTM_HYPERPARAMS = False\n",
    "\n",
    "### Model Training\n",
    "TRAIN_STACKED_LSTM = True\n",
    "TRAIN_STACKED_BILSTM = True\n",
    "TRAIN_ENSEMBLE_STACKED_BILSTM = False\n",
    "\n",
    "NUM_EPOCHS = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetch Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US EIA NYMEX Dataset\n",
    "TODO EXPLAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csvs to dataframes\n",
    "nymex_spot_df = pd.read_csv(\"data/US_EIA_NYMEX_spot.csv\")\n",
    "nymex_futures_df = pd.read_csv(\"data/US_EIA_NYMEX_futures.csv\")\n",
    "\n",
    "# Turn Date column into a datetime object rather than string\n",
    "nymex_spot_df[\"Date\"] = nymex_spot_df[\"Date\"].map(lambda str : datetime.strptime(str, \"%b %d, %Y\"))\n",
    "nymex_futures_df[\"Date\"] = nymex_futures_df[\"Date\"].map(lambda str : datetime.strptime(str, \"%b %d, %Y\"))\n",
    "\n",
    "# Trim to Jan 2013 - Jun 2019\n",
    "nymex_spot_df = nymex_spot_df[(nymex_spot_df[\"Date\"] >= datetime(year=2004, month=1, day=1)) & (nymex_spot_df[\"Date\"] <= datetime(year=2019, month=6, day=28))]\n",
    "nymex_futures_df = nymex_futures_df[(nymex_futures_df[\"Date\"] >= datetime(year=2004, month=1, day=1)) & (nymex_futures_df[\"Date\"] <= datetime(year=2019, month=6, day=28))]\n",
    "\n",
    "# Reset index\n",
    "nymex_spot_df = nymex_spot_df.set_index(\"Date\")\n",
    "nymex_futures_df = nymex_futures_df.set_index(\"Date\")\n",
    "\n",
    "# Rename Columns\n",
    "nymex_spot_df    = nymex_spot_df.rename(columns={'Henry Hub Natural Gas Spot Price (Dollars per Million Btu)': 'Spot Price'})\n",
    "nymex_futures_df = nymex_futures_df.rename(columns={'Natural Gas Futures Contract 1 (Dollars per Million Btu)':'Futures 1 Price','Natural Gas Futures Contract 2 (Dollars per Million Btu)':'Futures 2 Price','Natural Gas Futures Contract 3 (Dollars per Million Btu)':'Futures 3 Price','Natural Gas Futures Contract 4 (Dollars per Million Btu)':'Futures 4 Price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_spot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_futures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTrends Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrends\n",
    "from pytrends.request import TrendReq\n",
    "import time\n",
    "pytrend = TrendReq()\n",
    "\n",
    "def get_daily_trends_as_df(keywords, categories, timeframe, countries, search_type):\n",
    "    # Get pytrend suggestions and store them in exact_keywords\n",
    "    keywords_codes = [pytrend.suggestions(keyword=i)[0] for i in keywords] \n",
    "    df_CODES= pd.DataFrame(keywords_codes)\n",
    "    exact_keywords = df_CODES['mid'].to_list()\n",
    "\n",
    "    # Store keywords alongside their respective categories\n",
    "    individual_exact_keyword = list(zip(*[iter(exact_keywords)]*1, categories))\n",
    "    individual_exact_keyword = [list(x) for x in individual_exact_keyword]\n",
    "\n",
    "    # Split timeframe into 1 month chunks\n",
    "    timeframe_start, timeframe_end = timeframe.split(\" \")\n",
    "    start_date = date.fromisoformat(timeframe_start)\n",
    "    end_date = date.fromisoformat(timeframe_end)\n",
    "    dates = []\n",
    "    while start_date < end_date:\n",
    "        dates.append(start_date)\n",
    "        start_date = start_date + relativedelta(months=6)\n",
    "    dates.append(end_date + relativedelta(days=1))\n",
    "\n",
    "    # Compute number of fetches required\n",
    "    max_fetches = len(countries) * len(keywords) * (len(dates) - 1)\n",
    "    curr_fetches = 0\n",
    "\n",
    "    word_id = 0\n",
    "    trend_dict = {}\n",
    "    for country in countries:\n",
    "        for keyword, category in individual_exact_keyword:\n",
    "            try:\n",
    "                trend_dict[keyword] = pd.read_csv(f\"data/google_trends_series/{keywords[word_id]}.csv\")\n",
    "                trend_dict[keyword] = trend_dict[keyword].set_index(\"Date\")\n",
    "                curr_fetches += (len(dates) - 1)\n",
    "                print(\"[\" + \"=\" * math.floor(20 * curr_fetches / max_fetches - 1) +  \">\" * min(math.floor(20 * curr_fetches / max_fetches), 1) + \" \" * (20 - math.floor(20 * curr_fetches / max_fetches)) + \"] \",end='')\n",
    "                print(f\"{math.floor(10000 * curr_fetches / max_fetches)/100}% Complete!\", end='\\r')\n",
    "\n",
    "            except:\n",
    "                trend_dict[keyword] = pd.DataFrame()\n",
    "                # Fetch overall google trends for normalizing\n",
    "                pytrend.build_payload(kw_list=[keyword], \n",
    "                                        timeframe = timeframe, \n",
    "                                        geo = country, \n",
    "                                        cat=category,\n",
    "                                        gprop=search_type)\n",
    "                overall_data = pytrend.interest_over_time()\n",
    "                time.sleep(5)\n",
    "                month_num = 0\n",
    "                for i in range(len(dates)-1):\n",
    "                    pytrend.build_payload(kw_list=[keyword], \n",
    "                                        timeframe = dates[i].isoformat() + \" \" + (dates[i + 1] - relativedelta(days=1)).isoformat(), \n",
    "                                        geo = country, \n",
    "                                        cat=category,\n",
    "                                        gprop=search_type)\n",
    "                    curr_fetches += 1\n",
    "                    print(\"[\" + \"=\" * math.floor(20 * curr_fetches / max_fetches - 1) +  \">\" * min(math.floor(20 * curr_fetches / max_fetches), 1) + \" \" * (20 - math.floor(20 * curr_fetches / max_fetches)) + \"] \",end='')\n",
    "                    print(f\"{math.floor(10000 * curr_fetches / max_fetches)/100}% Complete! [\\\"{keywords[word_id]}\\\" {dates[i]}-{dates[i+1] - relativedelta(days=1)}]\", end='\\r')\n",
    "                    time.sleep(5) # sleep to prevent google shutting us down\n",
    "                    month_data = pytrend.interest_over_time()\n",
    "\n",
    "                    # normalize data based on overall_data\n",
    "                    for month in range(0,6):\n",
    "                        if month != 5:\n",
    "                            indicies = month_data.index.intersection(pd.date_range(dates[i] + relativedelta(months=month), (dates[i] + relativedelta(months=month+1)) - relativedelta(days=1), freq='d'))\n",
    "                        else:\n",
    "                            indicies = month_data.index.intersection(pd.date_range(dates[i] + relativedelta(months=month), dates[i+1] - relativedelta(days=1), freq='d'))\n",
    "                        month_data.loc[indicies] = month_data.loc[indicies] * (overall_data.iloc[month_num][keyword].mean() / month_data.loc[indicies].mean())\n",
    "                        month_num += 1\n",
    "\n",
    "                    # add data to trend_dict then delete for mem purposes\n",
    "                    trend_dict[keyword] = pd.concat([trend_dict[keyword], month_data], axis=0)\n",
    "                    del month_data\n",
    "                del overall_data\n",
    "\n",
    "                # fix up data and save partial data\n",
    "                trend_dict[keyword] = trend_dict[keyword].drop('isPartial', axis=1)\n",
    "                trend_dict[keyword] = trend_dict[keyword].reset_index(level=0)\n",
    "                trend_dict[keyword] = trend_dict[keyword].rename(columns={'date': 'Date', keyword:keywords[word_id]})\n",
    "                trend_dict[keyword] = trend_dict[keyword].set_index(\"Date\")\n",
    "                trend_dict[keyword].to_csv(f\"data/google_trends_series/{keywords[word_id]}.csv\")\n",
    "            word_id+=1\n",
    "\n",
    "    df_trends = pd.concat(trend_dict, axis=1)\n",
    "    df_trends.columns = df_trends.columns.droplevel(0) #drop outside header\n",
    "\n",
    "    return df_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    google_trends_df = pd.read_csv(\"data/google_trends_dataset.csv\")\n",
    "    google_trends_df = google_trends_df.set_index(\"Date\")\n",
    "    google_trends_df.index = pd.to_datetime(google_trends_df.index)\n",
    "except:\n",
    "    google_trends_df = get_daily_trends_as_df(KEYWORDS, KEYWORDS_CATEGORIES, TIMEFRAME, COUNTRIES, SEARCH_TYPE)\n",
    "    google_trends_df.index = pd.to_datetime(google_trends_df.index)\n",
    "    google_trends_df.to_csv(\"data/google_trends_dataset.csv\")\n",
    "\n",
    "google_trends_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a `plot` function that will be able to plot our data with, or without label and with or without predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, units=None, label_width=0, predictions=None, density=1, seperate=True, file=None, label_dates=[], labels=[]):\n",
    "    \"\"\"\n",
    "    data: of the form of a dataframe, indexed by a datetime object, each column being a seperate series to be plotted\n",
    "    units: if seperate is True, a list of strings, else a string\n",
    "    label_width: label width of time series window\n",
    "    predictions: of the form of a dataframe, indexed by a datetime object, each column being a seperate series to be plotted (should match up with data column names)\n",
    "    density: average every 'density' number of entries to end up plotting 1/density as many entries\n",
    "    file: file to save figure to (None if no save)\n",
    "    \"\"\"\n",
    "    # Verify units\n",
    "    if units==None:\n",
    "        units = [\"\"] * len(data.columns)\n",
    "    elif isinstance(units, str):\n",
    "        units = [units] * len(data.columns)\n",
    "    elif seperate and len(units) != len(data.columns):\n",
    "        print(\"ERROR: Make sure units is the same length as data\")\n",
    "        return\n",
    "\n",
    "    # Create new data if density != 1 using the mean of rolling windows\n",
    "    if density != 1:\n",
    "        # data\n",
    "        data = data.rolling(density).mean().iloc[::density,:]\n",
    "\n",
    "        # predictions\n",
    "        if predictions is not None:\n",
    "            predictions = predictions.rolling(density).mean().loc[data.index.intersection(predictions.index),:]\n",
    "\n",
    "    # Create a 12x12 figure\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    # If plotting seperate Divide the fig into N subplots where N is the number of columns in data\n",
    "    if seperate:\n",
    "        ncols=math.ceil(math.sqrt(len(data.columns)))\n",
    "        nrows=math.ceil( len(data.columns) / ncols)\n",
    "    else: \n",
    "        plt.ylabel(units[0])\n",
    "        plt.xlabel(\"Date\")\n",
    "    i = 1\n",
    "\n",
    "    # Iterate over all columns in data\n",
    "    for column, unit in zip(data.columns, units):\n",
    "        # Label seperate subfigs\n",
    "        if seperate:\n",
    "            ax = plt.subplot(nrows,ncols,i)\n",
    "            ax.set_title(column)\n",
    "            plt.ylabel(unit)\n",
    "            plt.xlabel('Date')\n",
    "            plt.locator_params(axis='x', nbins=10)\n",
    "        i+=1\n",
    "\n",
    "        # Plot data\n",
    "        plt.plot(data.index,data[column], label=column)\n",
    "\n",
    "        # Plot labels\n",
    "        if column in labels:\n",
    "            plt.plot(data.loc[label_dates.intersection(data.index)][column], label=column + \" labels\" )\n",
    "\n",
    "        # Plot predictions\n",
    "        if label_width != 0 and column in predictions.columns:\n",
    "            # plot as one long series if label width is 1\n",
    "            if label_width == 1:\n",
    "                plt.plot(predictions.index, predictions[column], label=column + \" Predictions\")\n",
    "\n",
    "            # plot as a lot of series if label width is > 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        if seperate:\n",
    "            plt.legend()\n",
    "\n",
    "    if not seperate:\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if file:\n",
    "        plt.savefig(file)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYMEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join dataframes (by date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_df = pd.concat([nymex_spot_df, nymex_futures_df],axis=1)\n",
    "nymex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some NaN values due to the dates covered differing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill spot price voids \n",
    "Interpolate all voids linearly (by column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_df = nymex_df.interpolate(method='time')\n",
    "nymex_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate Weekends and Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_df = nymex_df.reindex(pd.date_range(nymex_df.index[0],nymex_df.index[-1],freq='d'))\n",
    "nymex_df.index.names = [\"Date\"]\n",
    "nymex_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_df = nymex_df.interpolate(method='time')\n",
    "nymex_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot NYMEX Data (note the exponential nature to it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for column in nymex_df.columns:\n",
    "    if i == 0:\n",
    "        plot(nymex_df[[column]], units=\"(Dollars per Million Btu)\", density=1, file=TOP_FOLDER_NAME+\"images/nymex_data_\" + column.replace(\" \",\"_\") + \".png\")\n",
    "    else:\n",
    "        plot(nymex_df[[column]], units=\"(Dollars per Million Btu)\", density=1, file=TOP_FOLDER_NAME+\"images/nymex_data_\" + column.replace(\" \",\"_\") + \".png\", labels=[column], label_dates=pd.date_range(datetime(year=2013,month=1,day=1),datetime(year=2019,month=6,day=28),freq='d'))\n",
    "    i+=1\n",
    "\n",
    "plot(\n",
    "    nymex_df,\n",
    "    units=\"(Dollars per Million Btu)\", \n",
    "    seperate=True, \n",
    "    density=1, \n",
    "    file=TOP_FOLDER_NAME+\"images/nymex_data.png\", \n",
    "    labels=nymex_df.columns[1:], \n",
    "    label_dates=pd.date_range(datetime(year=2013,month=1,day=1),datetime(year=2019,month=6,day=28),freq='d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize columns\n",
    "First log all values: $x' = \\log{x}$, then normalize: $x' = (x - \\mu)/\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "stds = {} \n",
    "\n",
    "def normalize_nymex(nymex_df):\n",
    "    assert (nymex_df <= 0).sum().sum() == 0\n",
    "    global means\n",
    "    global stds\n",
    "\n",
    "    nymex_df_normalized = np.log(nymex_df)\n",
    "    for column in nymex_df_normalized.columns:\n",
    "        means[column] = nymex_df_normalized[column].mean()\n",
    "        stds[column] = nymex_df_normalized[column].std()\n",
    "        nymex_df_normalized[column] = (nymex_df_normalized[column] - means[column]) / stds[column]\n",
    "    \n",
    "    return nymex_df_normalized\n",
    "\n",
    "def restore_nymex(nymex_df_normalized):\n",
    "    global means\n",
    "    global stds\n",
    "\n",
    "    for column in nymex_df_normalized:\n",
    "        nymex_df_normalized[column] = nymex_df_normalized[column] * stds[column] + means[column]\n",
    "    nymex_df = np.exp(nymex_df_normalized)\n",
    "    return nymex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log each value and Normalize each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nymex_df_normalized = normalize_nymex(nymex_df)\n",
    "nymex_df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for column in nymex_df.columns:\n",
    "    if i == 0:\n",
    "        plot(nymex_df_normalized[[column]], units=\"(Log Normalized Cost)\", density=1, file=TOP_FOLDER_NAME+\"images/nymex_data_\" + column.replace(\" \",\"_\") + \"_normalized.png\")\n",
    "    else:\n",
    "        plot(nymex_df_normalized[[column]], units=\"(Log Normalized Cost)\", density=1, file=TOP_FOLDER_NAME+\"images/nymex_data_\" + column.replace(\" \",\"_\") + \"_normalized.png\", labels=[column], label_dates=pd.date_range(datetime(year=2013,month=1,day=1),datetime(year=2019,month=6,day=28),freq='d'))\n",
    "    i+=1\n",
    "\n",
    "plot(\n",
    "    nymex_df_normalized,\n",
    "    units=\"(Log Normalized Cost)\", \n",
    "    seperate=True, \n",
    "    density=1, \n",
    "    file=TOP_FOLDER_NAME+\"images/nymex_data_normalized.png\", \n",
    "    labels=nymex_df.columns[1:], \n",
    "    label_dates=pd.date_range(datetime(year=2013,month=1,day=1),datetime(year=2019,month=6,day=28),freq='d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Google Trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in google_trends_df.columns:\n",
    "    plot(google_trends_df[[column]], units=\"Search Volume\", density=30, file=TOP_FOLDER_NAME+\"images/google_trends_data_\" + column.replace(\" \",\"_\") + \".png\")\n",
    "\n",
    "plot(google_trends_df,units=\"Search Volume\", seperate=True, density=30, file=TOP_FOLDER_NAME+\"images/google_trends_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Columns \n",
    "$x' = (x- \\mu)/\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_df_normalized = google_trends_df.copy()\n",
    "\n",
    "for column in google_trends_df_normalized.columns:\n",
    "    google_trends_df_normalized[column] = (google_trends_df_normalized[column] - google_trends_df_normalized[column].mean()) / google_trends_df_normalized[column].std()\n",
    "\n",
    "google_trends_df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate / Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_df_normalized = google_trends_df_normalized.interpolate(method='time')\n",
    "google_trends_df_normalized = google_trends_df_normalized.fillna(0)\n",
    "google_trends_df_normalized.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in google_trends_df.columns:\n",
    "    plot(google_trends_df_normalized[[column]], units=\"Search Volume\", density=30, file=TOP_FOLDER_NAME+\"images/google_trends_data_\" + column.replace(\" \",\"_\") + \"_normalized.png\")\n",
    "\n",
    "plot(google_trends_df_normalized,units=\"Normalized Search Volume\", seperate=True, density=30, file=TOP_FOLDER_NAME+\"images/google_trends_data_normalized.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join individual dataframes into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([nymex_df_normalized, google_trends_df_normalized],axis=1).loc[nymex_df_normalized.index]\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to Window Dataset function (batching compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_df_to_ds(df, features, labels, feature_width=7, label_width=1, label_dates=None):\n",
    "    \"\"\"\n",
    "    df : dataframe with a datetime index\n",
    "    features : columns of df designated to be features\n",
    "    labels : columns of df designmated to be labels\n",
    "    feature_width : time-width of features in window\n",
    "    label_width : time-width of labels in window\n",
    "    label_dates : restrict the labels to come from this index\n",
    "    \"\"\"\n",
    "    if label_dates is None:\n",
    "        label_dates = df.index[feature_width:-(label_width)]\n",
    "\n",
    "    def feature_gen():\n",
    "        for label_start_date in label_dates:\n",
    "            new_feature = df.loc[df.index.intersection(pd.date_range(label_start_date - relativedelta(days=feature_width), label_start_date - relativedelta(days=1), freq='d'))][features].values\n",
    "            yield tf.convert_to_tensor(new_feature, dtype=np.float64)\n",
    "    \n",
    "    def label_gen():\n",
    "        for label_start_date in label_dates:\n",
    "            new_label = df.loc[df.index.intersection(pd.date_range(label_start_date, label_start_date + relativedelta(days=label_width - 1), freq='d'))][labels].values\n",
    "            yield tf.convert_to_tensor(new_label, dtype=np.float64)\n",
    "\n",
    "    # Turn np arrays into tf datasets\n",
    "    feature_dataset = tf.data.Dataset.from_generator(feature_gen,\n",
    "                                                     output_signature=(tf.TensorSpec(shape=(feature_width, len(features)), dtype=np.float64, name='Feature'))\n",
    "                                                    )\n",
    "    label_dataset = tf.data.Dataset.from_generator(label_gen,\n",
    "                                                     output_signature=(tf.TensorSpec(shape=(label_width, len(labels)), dtype=np.float64, name='Label'))\n",
    "                                                    )\n",
    "\n",
    "    # Zip datasets together into feature, label pairs\n",
    "    dataset = tf.data.Dataset.zip((feature_dataset, label_dataset))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to variable length input Dataset (batching not compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_df_to_ds(df, features, labels, label_width=1, label_dates=None):\n",
    "    \"\"\"\n",
    "    df : dataframe with a datetime index\n",
    "    features : columns of df designated to be features\n",
    "    labels : columns of df designmated to be labels\n",
    "    label_width : time-width of labels in window\n",
    "    label_dates : restrict the labels to come from this index\n",
    "    \"\"\"\n",
    "    if label_dates is None:\n",
    "        label_dates = df.index[1:-(label_width)]\n",
    "\n",
    "    start_date = df.index[0]\n",
    "    end_date = df.index[-1]\n",
    "\n",
    "    # Create feature generator\n",
    "    def feature_gen():\n",
    "        for label_start_date in label_dates:\n",
    "            new_feature = df.loc[df.index.intersection(pd.date_range(start_date, label_start_date - relativedelta(days=1), freq='d'))][features].values\n",
    "            yield tf.convert_to_tensor(new_feature, dtype=np.float64)\n",
    "\n",
    "    def label_gen():\n",
    "        for label_start_date in label_dates:\n",
    "            new_label = df.loc[df.index.intersection(pd.date_range(label_start_date, label_start_date + relativedelta(days=label_width - 1), freq='d'))][labels].values\n",
    "            yield tf.convert_to_tensor(new_label, dtype=np.float64)\n",
    "\n",
    "    \n",
    "    # Turn np arrays into tf datasets\n",
    "    feature_dataset = tf.data.Dataset.from_generator(feature_gen,\n",
    "                                                     output_signature=(tf.TensorSpec(shape=(None, len(features)), dtype=np.float64, name='Feature'))\n",
    "                                                    )\n",
    "    label_dataset = tf.data.Dataset.from_generator(label_gen,\n",
    "                                                     output_signature=(tf.TensorSpec(shape=(label_width, len(labels)), dtype=np.float64, name='Label'))\n",
    "                                                    )\n",
    "\n",
    "    # Zip datasets together into feature, label pairs\n",
    "    dataset = tf.data.Dataset.zip((feature_dataset, label_dataset))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to variable length input Dataset (batching compatible) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_batched_df_to_ds(df, features, labels, label_width=1, label_dates=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    df : dataframe with a datetime index\n",
    "    features : columns of df designated to be features\n",
    "    labels : columns of df designmated to be labels\n",
    "    label_width : time-width of labels in window\n",
    "    label_dates : restrict the labels to come from this index\n",
    "    \"\"\"\n",
    "    if label_dates is None:\n",
    "        label_dates = df.index[1:-(label_width)]\n",
    "\n",
    "    start_date = df.index[0]\n",
    "    end_date = df.index[-1]\n",
    "\n",
    "    # Create feature generator\n",
    "    def feature_gen():\n",
    "        part_of_batch = 0\n",
    "        for label_start_date in label_dates:\n",
    "            new_feature = df.loc[df.index.intersection(pd.date_range(start_date + relativedelta(days=part_of_batch), label_start_date - relativedelta(days=1), freq='d'))][features].values\n",
    "            yield tf.convert_to_tensor(new_feature, dtype=np.float64)\n",
    "            part_of_batch = 0 if part_of_batch == 31 else part_of_batch+1\n",
    "\n",
    "    def label_gen():\n",
    "        for label_start_date in label_dates:\n",
    "            new_label = df.loc[df.index.intersection(pd.date_range(label_start_date, label_start_date + relativedelta(days=label_width - 1), freq='d'))][labels].values\n",
    "            yield tf.convert_to_tensor(new_label, dtype=np.float64)\n",
    "\n",
    "    \n",
    "    # Turn np arrays into tf datasets\n",
    "    feature_dataset = tf.data.Dataset.from_generator(feature_gen,\n",
    "                                                     output_signature=(tf.TensorSpec(shape=(None, len(features)), dtype=np.float64, name='Feature'))\n",
    "                                                    )\n",
    "    label_dataset = tf.data.Dataset.from_generator(label_gen,\n",
    "                                                     output_signature=(tf.TensorSpec(shape=(label_width, len(labels)), dtype=np.float64, name='Label'))\n",
    "                                                    )\n",
    "\n",
    "    # Zip datasets together into feature, label pairs\n",
    "    dataset = tf.data.Dataset.zip((feature_dataset, label_dataset))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validate Test Dataset Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, batch_size=8, repeats=1, ds_size=None):\n",
    "    \"\"\"\n",
    "    ds: tensorflow zip dataset\n",
    "    train_split : \n",
    "    val_split :\n",
    "    test_split :\n",
    "    shuffle : whether to shuffle the ds or not\n",
    "    \"\"\"\n",
    "    assert (train_split + val_split + test_split) == 1\n",
    "    shuffle_size=10000\n",
    "\n",
    "    # First batch\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # Then shuffle the batches\n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=0)\n",
    "    \n",
    "    if ds_size is None:\n",
    "        ds_size = len(ds)\n",
    "    \n",
    "    train_size = math.ceil(int(train_split * ds_size) / batch_size)\n",
    "    val_size = math.ceil(int(val_split * ds_size) / batch_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training, Validation, and Testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIMEFRAME is None:\n",
    "    TIMEFRAME = full_df.index\n",
    "\n",
    "if DATASET_TYPE == \"window\":\n",
    "    dataset = window_df_to_ds(full_df.loc[TIMEFRAME], features=FEATURES, labels=LABELS, feature_width=FEATURE_WIDTH, label_width=LABEL_WIDTH, label_dates=LABEL_DATES)\n",
    "elif DATASET_TYPE == \"variable\" and BATCH_SIZE==1:\n",
    "    dataset = variable_df_to_ds(full_df.loc[TIMEFRAME], features=FEATURES, labels=LABELS, label_width=LABEL_WIDTH, label_dates=LABEL_DATES)\n",
    "else:\n",
    "    dataset = variable_batched_df_to_ds(full_df.loc[TIMEFRAME], features=FEATURES, labels=LABELS, label_width=LABEL_WIDTH, label_dates=LABEL_DATES, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_ds, val_ds, test_ds = train_val_test_split(dataset, train_split=TRAIN_SPLIT, val_split=VAL_SPLIT, test_split=TEST_SPLIT, batch_size=BATCH_SIZE, repeats=REPEATS, ds_size=len(LABEL_DATES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in dataset.take(33):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Stacked LSTM Model\n",
    "Parameters:\n",
    "- num_lstm_layers: Number of stacked LSTM layers\n",
    "- num_lstm_nodes: Number of LSTM nodes per LSTM layer\n",
    "- num_dense_layers: Number of stacked Dense layers to follow the LSTM layers\n",
    "- num_dense_nodes: Number of Dense nodes per Dense layer\n",
    "- dropout_rate: Fraction of weights to cut randomly before each dense layer\n",
    "- input_shape: Input Shape (without batching)\n",
    "- output_shape: Output Shape (wihtout batching)\n",
    "- learning_rate: Learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_lstm_model(num_lstm_layers, num_lstm_nodes, num_dense_layers, num_dense_nodes, dropout_rate, input_shape, output_shape, learning_rate, beta_1):\n",
    "    # Init model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape, name=\"Input\"))\n",
    "\n",
    "    # Add stacked LSTM layers\n",
    "    for i in range(num_lstm_layers):\n",
    "        if i < num_lstm_layers-1:\n",
    "            model.add(tf.keras.layers.LSTM(units=num_lstm_nodes, input_shape=input_shape, return_sequences=True, name=f'LSTM_{i}'))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.LSTM(units=num_lstm_nodes, input_shape=input_shape, return_sequences=False, name=f'LSTM_{i}'))\n",
    "\n",
    "    # Add stacked dense layers\n",
    "    for i in range(num_dense_layers):\n",
    "        # Add dropout layer\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate, name=f\"Dropout_{i}\"))\n",
    "        # Add dense layer\n",
    "        model.add(tf.keras.layers.Dense(units=num_dense_nodes, name=f\"Dense_{i}\"))\n",
    "\n",
    "    # Add output layer + reshape\n",
    "    model.add(tf.keras.layers.Dense(units=output_shape[0] * output_shape[1]))\n",
    "    model.add(tf.keras.layers.Reshape(target_shape=output_shape))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError(),tf.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    # Return model (pre-compiled)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked BiLSTM Model\n",
    "- num_lstm_layers: Number of stacked LSTM layers\n",
    "- num_lstm_nodes: Number of LSTM nodes per LSTM layer\n",
    "- num_dense_layers: Number of stacked Dense layers to follow the LSTM layers\n",
    "- num_dense_nodes: Number of Dense nodes per Dense layer\n",
    "- dropout_rate: Fraction of weights to cut randomly before each dense layer\n",
    "- input_shape: Input Shape (without batching)\n",
    "- output_shape: Output Shape (wihtout batching)\n",
    "- learning_rate: Learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_bilstm_model(num_lstm_layers, num_lstm_nodes, num_dense_layers, num_dense_nodes, dropout_rate, input_shape, output_shape, learning_rate, beta_1):\n",
    "    # Init model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape, name=\"Input\"))\n",
    "\n",
    "    # Add stacked LSTM layers\n",
    "    for i in range(num_lstm_layers):\n",
    "        if i < num_lstm_layers-1:\n",
    "            model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=num_lstm_nodes, input_shape=input_shape, return_sequences=True, name=f'LSTM_{i}')))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=num_lstm_nodes, input_shape=input_shape, return_sequences=False, name=f'LSTM_{i}')))\n",
    "\n",
    "    # Add stacked dense layers\n",
    "    for i in range(num_dense_layers):\n",
    "        # Add dropout layer\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate, name=f\"Dropout_{i}\"))\n",
    "        # Add dense layer\n",
    "        model.add(tf.keras.layers.Dense(units=num_dense_nodes, name=f\"Dense_{i}\"))\n",
    "\n",
    "    # Add output layer + reshape\n",
    "    model.add(tf.keras.layers.Dense(units=output_shape[0] * output_shape[1]))\n",
    "    model.add(tf.keras.layers.Reshape(target_shape=output_shape))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError(),tf.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    # Return model (uncompiled)\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Stacked BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_stacked_bilstm_model():\n",
    "    # TODO Once models work\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_dim_num_lstm_layers = skopt.space.Integer(low=1, high=5, name='num_lstm_layers')\n",
    "lstm_dim_num_lstm_nodes = skopt.space.Integer(low=32, high=256, name='num_lstm_nodes', prior='log-uniform', base=2)\n",
    "#lstm_dim_num_dense_layers = skopt.space.Integer(low=1, high=3, name='num_dense_layers')\n",
    "lstm_dim_num_dense_nodes = skopt.space.Integer(low=256, high=2048, name='num_dense_nodes', prior='log-uniform', base=2)\n",
    "lstm_dim_dropout_rate = skopt.space.Real(low=0, high=1, prior='uniform', name='dropout_rate')\n",
    "lstm_dim_learning_rate = skopt.space.Real(low=1e-6, high=1e-1, prior='log-uniform', name='learning_rate', base=10)\n",
    "lstm_dim_beta_1 = skopt.space.Real(low=0.80, high=1, prior='uniform', name='beta_1')\n",
    "\n",
    "stacked_lstm_dimensions = [\n",
    "#    lstm_dim_num_lstm_layers,\n",
    "    lstm_dim_num_lstm_nodes,\n",
    "#    lstm_dim_num_dense_layers,\n",
    "    lstm_dim_num_dense_nodes,\n",
    "    lstm_dim_dropout_rate,\n",
    "    lstm_dim_learning_rate,\n",
    "    lstm_dim_beta_1\n",
    "]\n",
    "\n",
    "###\n",
    "# THE FOLLOWING FUNCTION WAS BASED OFF OF https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb\n",
    "###\n",
    "\n",
    "@skopt.utils.use_named_args(dimensions=stacked_lstm_dimensions)\n",
    "def eval_stacked_lstm_hyperparams(num_lstm_nodes, num_dense_nodes, dropout_rate, learning_rate, beta_1):\n",
    "    # Use current global variables for evaluation\n",
    "    global train_ds\n",
    "    global val_ds\n",
    "    global MAX_EPOCHS\n",
    "    global BATCH_SIZE\n",
    "    global best_loss\n",
    "    global INPUT_SHAPE\n",
    "    global OUTPUT_SHAPE\n",
    "    global search_num\n",
    "    global MAX_SEARCH\n",
    "\n",
    "    # Print current hyper-parameters\n",
    "    print('Search Num', search_num, \"/\", MAX_SEARCH)\n",
    "    print(\"-----------------------\")\n",
    "    #print('num_lstm_layers:', num_lstm_layers)\n",
    "    print('num_lstm_nodes:', num_lstm_nodes)\n",
    "    #print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('dropout_rate:', dropout_rate)\n",
    "    print('learning_rate:{0:.1e}'.format(learning_rate))\n",
    "    print('beta_1:', beta_1)\n",
    "    \n",
    "    search_num += 1\n",
    "\n",
    "    loss = np.inf\n",
    "    try:\n",
    "        # Create the model\n",
    "        model = create_stacked_lstm_model(5, num_lstm_nodes, 3, num_dense_nodes, dropout_rate, INPUT_SHAPE, OUTPUT_SHAPE, learning_rate, beta_1)\n",
    "\n",
    "        # Define early_stopping to not overrun\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(train_ds,\n",
    "                            epochs=MAX_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=val_ds,\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0)\n",
    "\n",
    "        # If loss is better than best loss, set best_loss to loss (and save the model)\n",
    "        loss = history.history['val_loss'][-1]\n",
    "\n",
    "        if loss < best_loss:\n",
    "            model.save(TOP_FOLDER_NAME+'models/hyperparam_search/stacked_lstm_best')\n",
    "            best_loss = loss\n",
    "\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if loss == np.inf or loss == np.NaN:\n",
    "        loss = 999999999999\n",
    "\n",
    "    # Print loss\n",
    "    print(f\"\\n|| val_loss: {loss} ||\\n\")\n",
    "\n",
    "    # Clear the Keras session\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    print()\n",
    "    # Return loss for skopt to minimize\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked BiLSTM Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilstm_dim_num_bilstm_layers = skopt.space.Integer(low=1, high=5, name='num_bilstm_layers')\n",
    "bilstm_dim_num_bilstm_nodes = skopt.space.Integer(low=32, high=128, name='num_bilstm_nodes', prior='log-uniform', base=2)\n",
    "#bilstm_dim_num_dense_layers = skopt.space.Integer(low=1, high=3, name='num_dense_layers')\n",
    "bilstm_dim_num_dense_nodes = skopt.space.Integer(low=256, high=2048, name='num_dense_nodes', prior='log-uniform', base=2)\n",
    "bilstm_dim_dropout_rate = skopt.space.Real(low=0, high=1, prior='uniform', name='dropout_rate')\n",
    "bilstm_dim_learning_rate = skopt.space.Real(low=1e-6, high=1e-1, prior='log-uniform', name='learning_rate', base=10)\n",
    "bilstm_dim_beta_1 = skopt.space.Real(low=0.80, high=1, prior='uniform', name='beta_1')\n",
    "\n",
    "stacked_bilstm_dimensions = [\n",
    "#    bilstm_dim_num_bilstm_layers,\n",
    "    bilstm_dim_num_bilstm_nodes,\n",
    "#    bilstm_dim_num_dense_layers,\n",
    "    bilstm_dim_num_dense_nodes,\n",
    "    bilstm_dim_dropout_rate,\n",
    "    bilstm_dim_learning_rate,\n",
    "    bilstm_dim_beta_1\n",
    "]\n",
    "\n",
    "###\n",
    "# THE FOLLOWING FUNCTION WAS BASED OFF OF https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb\n",
    "###\n",
    "\n",
    "@skopt.utils.use_named_args(dimensions=stacked_bilstm_dimensions)\n",
    "def eval_stacked_bilstm_hyperparams(num_bilstm_nodes, num_dense_nodes, dropout_rate, learning_rate, beta_1):\n",
    "    # Use current global variables for evaluation\n",
    "    global train_ds\n",
    "    global val_ds\n",
    "    global MAX_EPOCHS\n",
    "    global BATCH_SIZE\n",
    "    global best_loss\n",
    "    global INPUT_SHAPE\n",
    "    global OUTPUT_SHAPE\n",
    "    global search_num\n",
    "    global MAX_SEARCH \n",
    "\n",
    "    # Print current hyper-parameters\n",
    "    print('Search Num', search_num, \"/\", MAX_SEARCH)\n",
    "    print(\"-----------------------\")\n",
    "    #print('num_bilstm_layers:', num_bilstm_layers)\n",
    "    print('num_bilstm_nodes:', num_bilstm_nodes)\n",
    "    #print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('dropout_rate:', dropout_rate)\n",
    "    print('learning_rate:{0:.1e}'.format(learning_rate))\n",
    "    print('beta_1:', beta_1)\n",
    "    \n",
    "    search_num += 1\n",
    "\n",
    "    loss = np.inf\n",
    "    try:\n",
    "        # Create the model\n",
    "        model = create_stacked_bilstm_model(5, num_bilstm_nodes, 3, num_dense_nodes, dropout_rate, INPUT_SHAPE, OUTPUT_SHAPE, learning_rate, beta_1)\n",
    "\n",
    "        # Define early_stopping to not overrun\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(train_ds,\n",
    "                            epochs=MAX_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=val_ds,\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0)\n",
    "\n",
    "        # If loss is better than best loss, set best_loss to loss (and save the model)\n",
    "        loss = history.history['val_loss'][-1]\n",
    "\n",
    "        if loss < best_loss:\n",
    "            model.save(TOP_FOLDER_NAME+'models/hyperparam_search/stacked_bilstm_best')\n",
    "            best_loss = loss\n",
    "\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if loss == np.inf or loss == np.NaN:\n",
    "        loss = 999999999999\n",
    "\n",
    "    # Print loss\n",
    "    print(f\"\\n|| val_loss: {loss} ||\\n\")\n",
    "\n",
    "    # Clear the Keras session\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    print()\n",
    "    # Return loss for skopt to minimize\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Stacked BiLSTM Eval Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baesian Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_LSTM_HYPERPARAMS:\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/hyperparam_search/stacked_lstm_best\")\n",
    "        best_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "        print(f\"Inital best_loss from saved model: {best_loss}\")\n",
    "    except:\n",
    "        best_loss = np.inf\n",
    "        print(f\"Inital best_loss: {best_loss}\")\n",
    "\n",
    "    print(\"#########################\")\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    default_params = [32, 256, 0, 1e-3, .9]\n",
    "    search_num = 1\n",
    "    search_result = skopt.gp_minimize(func=eval_stacked_lstm_hyperparams,\n",
    "                                dimensions=stacked_lstm_dimensions,\n",
    "                                acq_func='EI', # Expected Improvement.\n",
    "                                n_calls=MAX_SEARCH,\n",
    "                                x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_LSTM_HYPERPARAMS:\n",
    "    axes = skopt.plots.plot_convergence(search_result)\n",
    "    axes.flatten()[0].figure.savefig(TOP_FOLDER_NAME+\"images/stacked_lstm_hyperparam_convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_LSTM_HYPERPARAMS:\n",
    "    dim_names = [\n",
    "    #    \"num_lstm_layers\",\n",
    "        \"num_lstm_nodes\",\n",
    "    #    \"num_dense_layers\",\n",
    "        \"num_dense_nodes\",\n",
    "        \"dropout_rate\",\n",
    "        \"learning_rate\",\n",
    "        \"beta_1\"\n",
    "    ]\n",
    "\n",
    "    axes = skopt.plots.plot.plot_objective(result=search_result, dimensions=dim_names)\n",
    "    axes.flatten()[0].figure.savefig(TOP_FOLDER_NAME+\"images/stacked_lstm_hyperparam_objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_LSTM_HYPERPARAMS:\n",
    "    for i, name in enumerate(dim_names):\n",
    "        print(name[3:] + \": \" + str(search_result.x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_BILSTM_HYPERPARAMS:\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/hyperparam_search/stacked_bilstm_best\")\n",
    "        best_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "        print(f\"Inital best_loss from saved model: {best_loss}\")\n",
    "    except:\n",
    "        best_loss = np.inf\n",
    "        print(f\"Inital best_loss: {best_loss}\")\n",
    "\n",
    "    print(\"#########################\")\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    default_params = [1, 32, 3, 64, 0, 1e-3]\n",
    "    search_num = 1\n",
    "    search_result = skopt.gp_minimize(func=eval_stacked_bilstm_hyperparams,\n",
    "                                dimensions=stacked_bilstm_dimensions,\n",
    "                                acq_func='EI', # Expected Improvement.\n",
    "                                n_calls=MAX_SEARCH,\n",
    "                                x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_BILSTM_HYPERPARAMS:\n",
    "    axes = skopt.plots.plot_convergence(search_result)\n",
    "    axes.flatten()[0].figure.savefig(TOP_FOLDER_NAME+\"images/stacked_bilstm_hyperparam_convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_BILSTM_HYPERPARAMS:\n",
    "    dim_names = [\n",
    "    #    \"num_lstm_layers\",\n",
    "        \"num_bilstm_nodes\",\n",
    "    #    \"num_dense_layers\",\n",
    "        \"num_dense_nodes\",\n",
    "        \"dropout_rate\",\n",
    "        \"learning_rate\",\n",
    "        \"beta_1\"\n",
    "    ]\n",
    "\n",
    "    axes = skopt.plots.plot.plot_objective(result=search_result, dimensions=dim_names)\n",
    "    axes.flatten()[0].figure.savefig(TOP_FOLDER_NAME+\"images/stacked_bilstm_hyperparam_objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STACKED_BILSTM_HYPERPARAMS:\n",
    "    for i, name in enumerate(dim_names):\n",
    "        print(name[3:] + \": \" + str(search_result.x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_ENSEMBLE_STACKED_BILSTM_HYPERPARAMS:\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/hyperparam_search/ensemble_stacked_bilstm_best\")\n",
    "        best_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "        print(f\"Inital best_loss from saved model: {best_loss}\")\n",
    "    except:\n",
    "        best_loss = np.inf\n",
    "        print(f\"Inital best_loss: {best_loss}\")\n",
    "\n",
    "    print(\"#########################\")\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    default_params = [1, 32, 3, 64, 0, 1e-3]\n",
    "    search_num = 1\n",
    "    search_result = skopt.gp_minimize(func=eval_ensemble_stacked_bilstm_hyperparams,\n",
    "                                dimensions=ensemble_stacked_bilstm_dimensions,\n",
    "                                acq_func='EI', # Expected Improvement.\n",
    "                                n_calls=MAX_SEARCH,\n",
    "                                x0=default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_ENSEMBLE_STACKED_BILSTM_HYPERPARAMS:\n",
    "    axes = skopt.plots.plot_convergence(search_result)\n",
    "    axes.flatten()[0].figure.savefig(TOP_FOLDER_NAME+\"images/ensemble_stacked_bilstm_hyperparam_convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_ENSEMBLE_STACKED_BILSTM_HYPERPARAMS:\n",
    "    # TODO: Change these\n",
    "    dim_names = [\n",
    "    #    \"num_lstm_layers\",\n",
    "        \"num_bilstm_nodes\",\n",
    "    #    \"num_dense_layers\",\n",
    "        \"num_dense_nodes\",\n",
    "        \"dropout_rate\",\n",
    "        \"learning_rate\",\n",
    "        \"beta_1\"\n",
    "    ]\n",
    "\n",
    "    axes = skopt.plots.plot.plot_objective(result=search_result, dimensions=dim_names)\n",
    "    axes.flatten()[0].figure.savefig(TOP_FOLDER_NAME+\"images/ensemble_stacked_bilstm_hyperparam_objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_ENSEMBLE_STACKED_BILSTM_HYPERPARAMS:\n",
    "    for i, name in enumerate(dim_names):\n",
    "        print(name[3:] + \": \" + str(search_result.x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Best Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_STACKED_LSTM:\n",
    "    stacked_lstm = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/hyperparam_search/stacked_lstm_best\")\n",
    "    #stacked_lstm = create_stacked_lstm_model(5, 32, 3, 256, 0, INPUT_SHAPE, OUTPUT_SHAPE, learning_rate=1e-4, beta_1=.9)\n",
    "\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "            TOP_FOLDER_NAME+\"models/trained/lstm_stacked_best_32\",\n",
    "            monitor='val_loss',\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = stacked_lstm.fit(train_ds,\n",
    "                                initial_epoch=0,\n",
    "                                epochs=NUM_EPOCHS,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                validation_data=val_ds,\n",
    "                                callbacks=[save_best],\n",
    "                                verbose=1)\n",
    "\n",
    "    loss_values = history.history['loss']\n",
    "    epochs = range(1, len(loss_values)+1)\n",
    "    plt.plot(epochs, loss_values, label='Training Loss')\n",
    "    plt.gca().set_yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(TOP_FOLDER_NAME+\"images/stacked_lstm_loss_over_epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_STACKED_BILSTM:\n",
    "    stacked_bilstm = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/hyperparam_search/stacked_bilstm_best\")\n",
    "\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "            TOP_FOLDER_NAME+\"models/trained/bilstm_stacked_best\",\n",
    "            monitor='val_loss',\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = stacked_bilstm.fit(train_ds,\n",
    "                                epochs=NUM_EPOCHS,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                validation_data=val_ds,\n",
    "                                callbacks=[save_best],\n",
    "                                verbose=1)\n",
    "\n",
    "    loss_values = history.history['loss']\n",
    "    epochs = range(1, len(loss_values)+1)\n",
    "    plt.plot(epochs, loss_values, label='Training Loss')\n",
    "    plt.gca().set_yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(TOP_FOLDER_NAME+\"images/stacked_bilstm_loss_over_epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Ensemble BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_ENSEMBLE_STACKED_BILSTM:\n",
    "    ensemble_stacked_bilstm = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/hyperparam_search/ensemble_stacked_bilstm_best\")\n",
    "\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "            TOP_FOLDER_NAME+\"models/trained/bilstm_ensemble_stacked_best\",\n",
    "            monitor='val_loss',\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = ensemble_stacked_bilstm.fit(train_ds,\n",
    "                                epochs=NUM_EPOCHS,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                validation_data=val_ds,\n",
    "                                callbacks=[save_best],\n",
    "                                verbose=1)\n",
    "\n",
    "    loss_values = history.history['loss']\n",
    "    epochs = range(1, len(loss_values)+1)\n",
    "    plt.plot(epochs, loss_values, label='Training Loss')\n",
    "    plt.gca().set_yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(TOP_FOLDER_NAME+\"images/ensemble_stacked_bilstm_loss_over_epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions Function\n",
    "Returns predictions df from a given dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_df(model, dataset, label_width, labels, index):\n",
    "    predictions = np.ndarray((len(index), label_width, len(labels)))\n",
    "    dataset = dataset.batch(1)\n",
    "\n",
    "    for i, tensor in enumerate(iter(dataset)):\n",
    "        predictions[i,:,:] = model(tensor[0]).numpy()\n",
    "\n",
    "    # get rid of time dim\n",
    "    if label_width == 1:\n",
    "        predictions = predictions.reshape((len(index), len(labels)))\n",
    "    else: # compress time dim\n",
    "        pass\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions, index=index, columns=labels)\n",
    "    predictions_df = restore_nymex(predictions_df)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm = None\n",
    "try:\n",
    "    stacked_lstm = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/trained/stacked_lstm_best\")\n",
    "except:\n",
    "    print(\"No stacked_lstm model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stacked_lstm != None:\n",
    "    test_results[\"stacked_lstm\"] = dict(zip(stacked_lstm.metrics_names, stacked_lstm.evaluate(test_ds, verbose=0)))\n",
    "    test_results[\"stacked_lstm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stacked_lstm != None:\n",
    "    predictions_df = get_predictions_df(model=stacked_lstm, dataset=dataset, label_width=LABEL_WIDTH, labels=LABELS, index=LABEL_DATES)\n",
    "    for label in LABELS:\n",
    "        plot(nymex_df[[label]], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df[label], density=30, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_lstm_predictions_\" + label.replace(\" \", \"_\") + \"_sparse.png\")\n",
    "        plot(nymex_df[[label]], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df[label], density=1, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_lstm_predictions_\" + label.replace(\" \", \"_\") + \"_dense.png\")\n",
    "        \n",
    "\n",
    "    plot(nymex_df[LABELS], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df, density=30, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_lstm_predictions_sparse.png\")\n",
    "    plot(nymex_df[LABELS], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df, density=1, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_lstm_predictions_dense.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stacked_lstm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_bilstm = None\n",
    "try:\n",
    "    stacked_bilstm = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/trained/stacked_bilstm_best\")\n",
    "except:\n",
    "    print(\"No stacked_bilstm model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stacked_bilstm != None:\n",
    "    test_results[\"stacked_bilstm\"] = dict(zip(stacked_bilstm.metrics_names, stacked_bilstm.evaluate(test_ds, verbose=0)))\n",
    "    test_results[\"stacked_bilstm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stacked_bilstm != None:\n",
    "    predictions_df = get_predictions_df(model=stacked_bilstm, dataset=dataset, label_width=LABEL_WIDTH, labels=LABELS, index=LABEL_DATES)\n",
    "    for label in LABELS:\n",
    "        plot(nymex_df[[label]], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df[label], density=30, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_lstm_predictions_\" + label.replace(\" \", \"_\") + \"_sparse.png\")\n",
    "        plot(nymex_df[[label]], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df[label], density=1, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_lstm_predictions_\" + label.replace(\" \", \"_\") + \"_dense.png\")\n",
    "\n",
    "    plot(nymex_df[LABELS], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df, density=30, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_bilstm_predictions_sparse.png\")\n",
    "    plot(nymex_df[LABELS], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df, density=1, seperate=True, file=TOP_FOLDER_NAME+\"images/stacked_bilstm_predictions_dense.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stacked_bilstm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Stacked BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_stacked_bilstm = None\n",
    "try:\n",
    "    ensemble_stacked_bilstm = tf.keras.models.load_model(TOP_FOLDER_NAME+\"models/trained/ensemble_stacked_bilstm_best\")\n",
    "except:\n",
    "    print(\"No ensemble_stacked_bilstm model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ensemble_stacked_bilstm != None:\n",
    "    test_results[\"ensemble_stacked_bilstm\"] = dict(zip(ensemble_stacked_bilstm.metrics_names, ensemble_stacked_bilstm.evaluate(test_ds, verbose=0)))\n",
    "    test_results[\"ensemble_stacked_bilstm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ensemble_stacked_bilstm != None:\n",
    "    predictions_df = get_predictions_df(model=ensemble_stacked_bilstm, dataset=dataset, label_width=LABEL_WIDTH, labels=LABELS, index=LABEL_DATES)\n",
    "    for label in LABELS:\n",
    "        plot(nymex_df[[label]], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df[label], density=30, seperate=True, file=TOP_FOLDER_NAME+f\"images/stacked_lstm_predictions_\" + label.replace(\" \", \"_\") + \"_sparse.png\")\n",
    "        plot(nymex_df[[label]], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df[label], density=1, seperate=True, file=TOP_FOLDER_NAME+f\"images/stacked_lstm_predictions_\" + label.replace(\" \", \"_\") + \"_dense.png\")\n",
    "\n",
    "    plot(nymex_df[LABELS], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df, density=30, seperate=True, file=TOP_FOLDER_NAME+\"images/ensemble_stacked_bilstm_predictions_sparse.png\")\n",
    "    plot(nymex_df[LABELS], units=\"$$$\", label_width=LABEL_WIDTH, predictions=predictions_df, density=1, seperate=True, file=TOP_FOLDER_NAME+\"images/ensemble_stacked_bilstm_predictions_dense.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ensemble_stacked_bilstm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
